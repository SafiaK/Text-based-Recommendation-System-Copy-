 \subsection{Neural Network Based Recommendation Systems}
 In this paper\cite{N1} they used "title" and "abstract" sections to extract textual features and projected all the documents in such a way that a document is closed to its referenced papers. This projection of whole corpus is embedded once as a "documents embedding" and later used for query documents. The query document is projected on same vector space to find the nearest neighbours for candidate references. In second step of their recommendation system they used a trained neural network on textual fields of documents to find the score of each candidate returned in phase 1. The input to this neural network is a pair (dq,di) and output is score for each pair. At the end the documents are ranked based on this score.
 \\In a similar way \cite{N2} VOPRec learns structural vectors using Struc2vec algorithm and textual vectors using Doc2vec algorithm.After that, m-nearest text based and n-nearest structured based neighbours are connected to build a weighted citation network.They propose to find the paper similarity using network embedding based on that weighted citation network.
\\
\cite{N3}used Microsoft Knowledge graph to extract the neighbouring entities of an entity. Upon which they used deep learning techniques to recommend a news to user.also uses embeddings built a knowledge graph of entities. When a news comes in, they extract the entities in the news and also the neighbouring entities in the KG.They combined this knowledge-level news representation with word-level representation to design a component knowledge-aware convolution neural networks (KCNN) and and generated a knowledge-aware embedding vector.They then used an attention module to automatically match candidate news to each piece of clicked news, and aggregated the user’s history with different weights.For CTR predicition, candidate news' embedding and user’s embedding processed through a deep neural network (DNN).Unlike other deep learning recommendation model, KCNN is specialized for news recommendations.For experimentation, they used data from Bing news and Microsoft Satori knowledge graph to extract data.They used different settings for experimentation and were able to achieve F-MEASURE of 69% .
\\
\cite{N10}introduces a content based method that combines coverage score,popularity factor and the fact that if a news is click-bait or not, for the final recommendations. For content recommendations;after cleaning up the data and extracting the entities from news articles where every entity has a weight based on their frequency,they built up a graph model using the entity pairs.The weighted score of an entity is calculated by Page-Rank algorithm using this Graph. They combined this score with score they got by trained word embeddings and used it in the semantic vector representation.They used fast-text model to train these embeddings.The similarity score for this section is computed by calculating the cosine similarity of user's semantic vector with candidate news article.
Moreover, they trained classification model using Bi-LSTM to detect click-bait and predict popularity.Furthermore the coverage score cv(a) is computed which is based on the number of articles that cover the same news story (related articles in the same cluster) and the quality of their source.Finally,they combined the four types of score (calculated in the sections discussed above), i.e, the click bait cb(a) score of news a,cv(a) the coverage score of a,pp(a) is the predicted popularity score of a and ps(u,a)is the personalized score of the article (a) for user (u).
\\If the $cb>0.5$ then the final score would be 0, otherwise the final score would be $ps(u,a)+cv(a)+pp(a)$
\\For content base, they built a database of user's profile and kept the information of users'Id,entity Id, total clicks
,recent clicks and last click,for the popularity computation section they used Worldnews and Raddit dataset and for click-bait classification they used Reuters and The Examiner.
\\
\\\cite{N60} proposed a novel location-aware topic model based news recommendation system built on multilayered perceptron(MLP).They used every wikipedia concept as a Topic and to overcome the issues of sparsity, high dimnationality and redundancy; they propose DL based model called deep semantic analysis (DSA), which utilizes deep neural networks to map the (Wikipedia-concept-based) topic space to an abstract, dense, and low dimensional feature space, where the localized similarities between the users and their target news are maximized, and those with irrelevant news are minimized 
\cite{N12} proposed usage of Doc2Vec technique to generate embeddings on a cross lingual dataset of uncategorized news. These embeddings are then used for content based filtering to make recommendations. The result was then compared with LDA and LSA and it was proven that Doc2Vec outperforms both of these topic modeling techniques in term of accuracy. As the user data was missing they proposed to recommend news on bases of similarity of items in a fuzzy fashion as described in \cite{N12-2}
\\
Insted of only using explicit ratings, \cite{N13} used user reviews and comments for recommending. The proposed model SemRevRec to discover annotated and hidden entities from textual data which consists of reviews of music, books and movies from IMDb, LibraryThing and Amazon respectively. They used two semantic annotators AIDA and DBpedia Spotlight for rendering entities from their context. AIDA showed better accuracy in independat camparison. After finding annoted entities, they used SPARQL queries to retrieve relevant entities from two knowledge bases DBpedia and Wikidata for they both have Linked Data. After that they used these annoted and initial entities for recommendation and ranking. After comparing SemRevRec model with Random guess, Item KNN and Bayesian Personalized Ranking (BPR), they concluded that it outperforms them. Performance of this model is greater for Wikidata than DBpedia.
\\
\cite{N15} transforms the plain category structure of DBpedia into hierarchical taxonomy as prepossessing. In this way all entities are mapped into a hierarchy. After linking these entities to hierarchical categories, Explicit entity ratings given by the users are extended to categorical hierarchy. They used two spreading functions, Bell Log and Intersect booster, to score entities according to their hierarchical categories. Using content based recommendation approach, high ranked entities are added to top-n list. 
\\
Survey conducted in \cite{N23} emphasizes the role of review-based user profile building and review-based product profile building in recommender system. According to this survey, after semantic analysis of reviews to extract useful entities, these entities are also used with users explicit ratings for more accurate recommendation. 
\\
\cite{N31} proposed an architecture which implements a content-based recommender system able to predict a score s(u,i) which denotes the probability that a user u would like a specify an item i.
In a nutshell, given a user u ∈ U and an item i ∈ I, our approach executes two different pipelines, each of which learns a continuous vector representation for a different facet of i. The  first one, highlighted in grey, learns an embedding based on the textual description of the item. The second, highlighted in yellow, has the goal of learning an embedding based on the features gathered from the LOD cloud. Such embeddings are then merged through a concatenation layer with a placeholder identifying the current user u, and are finally exploited to feed a classifier which generates the preference estimation 0 ≤ s(u,i) ≤ 1
\\
\cite{N21} proposes the CTransR-CF algorithm based on knowledge graph embedding, which are fused in collaboration filtering technique to recommend Top-N predictions. They used movie-lens knowledge graph data set provided by “The Movie Database (TMDb)” website\footnote{https://www. themoviedb.org/}
\\
\cite{N24}proposes an architecture CHAMELEON for contextual session based recommendations.It composed of two modules the Article Content Representation (ACR) and the Next-Article Recommendation (NAR).The input of the ACR are the textual data of news article and its meta-data features. The module was trained for news category classification task. The internal learned embeddings during this process are used in NAR module afterwards.NAR module recommends the next article in the active session. The input of the module is learned news embedding (from ACR module) of last viewed article, the contextual property of news(popularity,recency) and user's geo location information. All these inputs are combined into user's embeddings.To cater with the issue of huge numbers of news articles, NAR model was trained to maximize the similairty between Predicted Next-Article Embedding and the User Embedding corresponding to the next article actually read by the user in his session (positive sample), whilst minimizing its similarity with negative samples (articles not read by the user during the session). With this strategy to deal with item cold-start, a newly published article might be immediately recommended, as soon as its Article Content Embedding is trained and added to a repository.  
\subsubsection{CNN}
\cite{N4} Content base Convolution Neural Network uses text features to recommend learning sources.The input of the network is LDA with number of topics = 100;latent factor model L1 as an output features and CNN as mapping function.Furthermore, they used split Bregman iteration method to solve the model.

\\
\cite{N11}extended their work presented in \cite{N33} by jointly training news encoder module for topic classification task as well.
\\
\cite{N14}uses textual, visual and structural information and built three types of embeddings using state of the art deep learning techniques.For textual data, they used Stacked denoising autoencoders\cite{N14-2} to learn embeddings.After then, they combined all these embeddings and built a matrix and applied collaborative filtering technique on this embedding matrix for recommendations.
\\
\cite{N66}proposed a combined learning model in which CNNs are used to learn to learn articles representation and attention is performed to deal with the diverse variance of editors’ selection behavior.
\subsubsection{Attention Network}
\subsubsection{DeepReinforcementLearning(DRL)}
\cite{N67}proposed a news recommendation system, DRN, using DRL to tackle three challenges: (i) dynamic changes of news content and user preference, (ii) incorporating return patterns (to the service) of users, and (iii) increasing the diversity of recommendations.
\subsubsection{Deep Learning Based KG Recommendation Systems}

\cite{N17} developed an RNN based knowledge graph embeddings;in which semantic path mining has done by adopting two strategies, i.e. they only considered user-item paths and they limit the path of the edges to 3,5, and 7.To lean the path representation and entity distribution,Attention-Gated Hidden Layer and Embedding Layer are added in the model architecture.Model is learnt by minimizing the Binary Cross Entropy  between the observed and estimated ratings.To recommend an item the score at the test time is computed by taking the inner product of user's and item's corresponding embeddings.
\cite{N33}
proposes a neural network based news recommender system which mainly constitutes of three modules, i.e., news encode, user encoder and click predictor.The purpose of news encoder is to learn news representation. It takes the “title” of news as its input; generates its embedding and feed these embedding to CNN, after that an attention model learns the important words in the title. The user-encoder module also uses the attention mechanism to learn the user representation using its browsed news. The click predictor is to predict the probability of a user clicking a candidate new. To train the whole model; concept of negative sampling is used. All the news articles clicked by users are considered as positive and K negative samples are taken in the same impression, which were not clicked by user. And in this way the news click prediction probability becomes a classification task where the loss function to compute recommendation for news is the negative log-likelihood of all positive samples.
\\
\subsubsection{Auto Encoders}
Given a poor quality description by service developer and mashup queries it is hard to suggest useful web services. The problem is catered by \cite{N59} by developing "DLTSR"( A Deep Learning Framework for Recommendations of Long-tail Web Services) with Stacked Denoising Auto-encoders as main building block.
 To tackle the problem of unsatisfactory quality of description given by service developers and mashup queries, we use the deep learning model SDAE as the basic component to learn ro- bust and effective representations. Moreover, the knowledge from usages in the hot service side is transferred and im- posed as a regularization on the output of SDAE to guide the learning of representations
 \subsubsection{RNN}
 \cite{p2} presented a GRUs based RNN model for news recommender system of Yahoo! News.The proved that usage of GRUs is more effective in learning of expressive aggregation of user history.The distributed representation of news articles were learning using auto-encoder in form of embeddings. The results show a significant improvement compared with the traditional word-based approach. The system has been fully deployed to online production services and serves more than 10 million unique users everyday.
 \\
 \cite{N62} extended the work of \cite{N64} by usage of RNN to add the temporal effectiveness in the user profiling.

\cite{N18} presented a multitask model which shared semantic query representation with entity recommendation and ranking. This model also handles in-session and ambiguous entities using previous search log of real time search engine for context analysis. For semantic understanding of queries, BiLSTM was used. They used neural network to map related entities and initial query into same vector so that semantic relevance would be determined using similarity function. 
\\
\cite{N19} discussed three memory based approaches for recommending entities. These domains are document-based, query-base and session-base which are used in collaborative filtering fashion to find the similarities between entities. They represented session as \textit{s}, query as \textit{q} and entity as \textit{e} and document-base approach observed user's previous record to determine relevancy of an entity \textit{e} to the session \textit{s} by comparing its similarity with all entities of same session. Query-based approach further personalize recommendations by finding relevant entities of the queries from all other previous sessions related to current session. Session-based approach uses Co-occurrence similarity and Centroid session similarity for the purpose of finding sessions similar to current session and recommend entities from those sessions. 




